{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Flowers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The `flower_photos` folder will be ignored"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB3\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             file_path   label\n",
      "0       flower_photos/roses/16209331331_343c899d38.jpg   roses\n",
      "1        flower_photos/roses/5777669976_a205f61e5b.jpg   roses\n",
      "2      flower_photos/roses/4860145119_b1c3cbaa4e_n.jpg   roses\n",
      "3       flower_photos/roses/15011625580_7974c44bce.jpg   roses\n",
      "4     flower_photos/roses/17953368844_be3d18cf30_m.jpg   roses\n",
      "...                                                ...     ...\n",
      "3665     flower_photos/tulips/134143359_71fa8dd9a4.jpg  tulips\n",
      "3666    flower_photos/tulips/3637371174_a8dfcc1b35.jpg  tulips\n",
      "3667  flower_photos/tulips/6948239566_0ac0a124ee_n.jpg  tulips\n",
      "3668    flower_photos/tulips/2834890466_1cf220fba1.jpg  tulips\n",
      "3669   flower_photos/tulips/13953090784_0c7d7a904e.jpg  tulips\n",
      "\n",
      "[3670 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for dir, x, files in os.walk(\"flower_photos\"):\n",
    "    label = dir.split('/')[-1]\n",
    "\n",
    "    for file in files:\n",
    "        path = os.path.join(dir, file)\n",
    "        data.append([path, label])\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"file_path\", \"label\"])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    validation_split = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2936 validated image filenames belonging to 5 classes.\n",
      "Found 734 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = datagen.flow_from_dataframe(\n",
    "    dataframe = df,\n",
    "    x_col = 'file_path',\n",
    "    y_col = 'label',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "valid_gen = datagen.flow_from_dataframe(\n",
    "    dataframe = df,\n",
    "    x_col = 'file_path',\n",
    "    y_col = 'label',\n",
    "    target_size = (224, 224),\n",
    "    color_mode = 'rgb',\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB3(\n",
    "    include_top= False, \n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3), \n",
    "    pooling='max'\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    base_model,\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(rate=.45, seed=123),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    Adamax(learning_rate= .0001), \n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'acc', \n",
    "        tf.keras.metrics.Precision(), \n",
    "        tf.keras.metrics.Recall(), \n",
    "        tf.keras.metrics.AUC()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    patience = 10,\n",
    "    min_delta = 0,\n",
    "    monitor = 'val_loss',\n",
    "    verbose = 0,\n",
    "    restore_best_weights = True,\n",
    "    baseline = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cohort23/Library/Python/3.10/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 3s/step - acc: 0.3692 - auc: 0.6992 - loss: 1.7290 - precision: 0.4102 - recall: 0.2914 - val_acc: 0.1076 - val_auc: 0.2703 - val_loss: 3.8999 - val_precision: 0.1333 - val_recall: 0.0981\n",
      "Epoch 2/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 3s/step - acc: 0.7509 - auc: 0.9395 - loss: 0.6918 - precision: 0.8025 - recall: 0.6865 - val_acc: 0.1199 - val_auc: 0.3006 - val_loss: 4.6585 - val_precision: 0.1328 - val_recall: 0.1158\n",
      "Epoch 3/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 3s/step - acc: 0.8325 - auc: 0.9724 - loss: 0.4619 - precision: 0.8805 - recall: 0.7947 - val_acc: 0.1199 - val_auc: 0.3132 - val_loss: 5.2122 - val_precision: 0.1254 - val_recall: 0.1144\n",
      "Epoch 4/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 3s/step - acc: 0.8536 - auc: 0.9799 - loss: 0.3897 - precision: 0.8870 - recall: 0.8263 - val_acc: 0.1185 - val_auc: 0.3177 - val_loss: 5.5807 - val_precision: 0.1254 - val_recall: 0.1172\n",
      "Epoch 5/5\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 3s/step - acc: 0.8930 - auc: 0.9884 - loss: 0.3025 - precision: 0.9228 - recall: 0.8697 - val_acc: 0.1185 - val_auc: 0.3300 - val_loss: 5.9260 - val_precision: 0.1237 - val_recall: 0.1172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x28672af20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_gen,\n",
    "    validation_data = valid_gen,\n",
    "    epochs = 5,\n",
    "    validation_steps = None,\n",
    "    shuffle = False,\n",
    "    callbacks = early_stopping,\n",
    "    verbose = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('flowers.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('daisy', 0), ('dandelion', 1), ('roses', 2), ('sunflowers', 3), ('tulips', 4)])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Your flower is a : 1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.saving import load_model\n",
    "import numpy as np\n",
    "\n",
    "model = load_model(\"flowers.keras\")\n",
    "\n",
    "input_image = cv2.imread('flower_photos/daisy/100080576_f52e8ee070_n.jpg')\n",
    "\n",
    "input_image_resize = cv2.resize(input_image, (224, 224))\n",
    "\n",
    "input_image_scaled = input_image_resize/225\n",
    "image_reshaped = np.reshape(input_image_scaled, [1,224,224,3])\n",
    "\n",
    "print(train_gen.class_indices.items())\n",
    "\n",
    "input_prediction = model.predict(image_reshaped)\n",
    "print(\"Your flower is a :\", np.argmax(input_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 10:44:50) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
